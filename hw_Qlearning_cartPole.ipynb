{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install dependancies, takes around 45 seconds\n",
        "\n",
        "Rendering Dependancies\n"
      ],
      "metadata": {
        "id": "LQN_pXbTV-q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "R8R_TaHTCY3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "f_5DjgoOWjw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPKs48i2A0o6"
      },
      "outputs": [],
      "source": [
        "# import some helper functions and tools\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import numpy as np\n",
        "import time, math, random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Cart-Pole consists of a pole, which is connected to a horizontally moving cart. T solve the task, the pole has to be balanced by applying a force F to the cart, The system is nonlinear, since the rotation of the pole introduces trigonometric functions into the force balance equations."
      ],
      "metadata": {
        "id": "jbsY1Qw_A9zL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # State and Action\n",
        " All observations are assigned a uniformly random value in (-0.05, 0.05)"
      ],
      "metadata": {
        "id": "kmQAHQgjB3RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "# The environment is old, there will be a warning. But there is no need to change the environment to a newer version.\n",
        "env.reset()\n",
        "# This will give you the initial states: [position, velocity, angle, angular velocity]"
      ],
      "metadata": {
        "id": "FBtQo_b7XS9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"upper bounds\", env.observation_space.high)\n",
        "print(\"lower bounds\", env.observation_space.low)"
      ],
      "metadata": {
        "id": "EQ6CTLCXb0uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **states** of the Cart-Pole are the distance s of the cart, the velocity dot $\\dot{s}$ of the cart, the angle of the pole $\\theta$ and the angular velocity of the pole dot $\\dot{\\theta}$. In the environment, the observation of the environment will be $\\text{obs}=[s,\\dot{s},\\theta,\\dot{\\theta}]$"
      ],
      "metadata": {
        "id": "RsNhb-yGA_qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "id": "8TEt1F12X9ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **action space** of the Cart-Pole environment is discrete, which includes 0 and 1. 0 means pushing the cart to the left, and 1 means pushing the cart to the right."
      ],
      "metadata": {
        "id": "JEJqtOziBHqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(reward)\n",
        "  if done:\n",
        "    break"
      ],
      "metadata": {
        "id": "SwN_rCsfYD8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reward:** Since the goal is to keep the pole upright for as long as possible, a reward of +1 for every step taken, including the termination step, is allotted. The threshold for rewards is 475"
      ],
      "metadata": {
        "id": "92o2EgKeBJcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more details, please refer to the documentation of [OpenAI gym](https://www.gymlibrary.dev/environments/classic_control/cart_pole/). "
      ],
      "metadata": {
        "id": "UJ8Od0fvBT03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Episode termination conditions"
      ],
      "metadata": {
        "id": "KoXKzrw4BWQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "for i in range(100):\n",
        "  action = env.action_space.sample()\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  if done:\n",
        "    print(obs)\n",
        "    if np.abs(obs[0]>2.4):\n",
        "      print('Cart Position is greater than 2.4!')\n",
        "    elif np.abs(obs[2]>0.2095):\n",
        "      print('Pole angle is greater than 12 degree!')\n",
        "    break"
      ],
      "metadata": {
        "id": "ZB3VyHtZYfok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The episode ends if any one of the following occurs:\n",
        "\n",
        "1. Termination: Pole Angle is greater than ±12°\n",
        "\n",
        "2. Termination: Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
        "\n",
        "3. Truncation: Episode length is greater than 500.\n"
      ],
      "metadata": {
        "id": "3OYKeZs_BaIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions for rendering"
      ],
      "metadata": {
        "id": "os1I5qsOZtNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "lT__1hO9BW3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = VideoRecorder(env, 'demo.mp4')\n",
        "  return env"
      ],
      "metadata": {
        "id": "aAW4jhquBfM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "video = wrap_env(env)\n",
        "obs = env.reset()\n",
        "i = 0\n",
        "while True:\n",
        "    i+=1\n",
        "    env.render()\n",
        "    video.capture_frame()\n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    obs, reward, done, info = env.step(action)   \n",
        "    if done: \n",
        "      if np.abs(obs[0]>2.4):\n",
        "        print('Cart Position is greater than 2.4!')\n",
        "      elif np.abs(obs[2]>0.2095):\n",
        "        print('Pole angle is greater than 12 degree!')\n",
        "      break\n",
        "video.close()            \n",
        "env.close()\n",
        "show_video()\n",
        "#right click the video, you can download it!"
      ],
      "metadata": {
        "id": "1dMGF1lkCKJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions begin here\n",
        "Now we already have a basic understanding of the environment, let's have more fun!"
      ],
      "metadata": {
        "id": "eooPb9aVau_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Q agent\n",
        "class Cart_Pole_Q_agent():\n",
        "  def __init__(self, discretization_bin=(1,1,8,16),\n",
        "                min_lr=0.1,\n",
        "                lr=0.2,\n",
        "                discount_factor = 0.99,\n",
        "                exploration_decay_rate =0.99,\n",
        "                exploration_rate =0.5,\n",
        "                num_episodes=1000):\n",
        "    # lr is short for learning rate, recall the update rule of Q learning, \n",
        "    #Q(s,a) += alpha*[R+gamma*max_aQ(s',a)-Q(s,a)]\n",
        "    #lr = alpha\n",
        "    #gamma = discount_factor\n",
        "    self.min_lr = min_lr\n",
        "    self.lr = lr\n",
        "    self.discount_factor = discount_factor\n",
        "    self.exploration_decay_rate = exploration_decay_rate\n",
        "    self.exploration_rate = exploration_rate\n",
        "    self.num_episodes = num_episodes\n",
        "    self.env = gym.make('CartPole-v0')\n",
        "    # Set the upper and lower bound\n",
        "    # Discretize the state space\n",
        "    self.discretization_bin = discretization_bin\n",
        "    self.upperbound = [2.4,3.0,0.5,2.0]\n",
        "    self.lowerbound = [-2.4,-3.0,-0.5,-2.0]\n",
        "    self.action_space_len = self.env.action_space.n\n",
        "    Q_table_size = self.discretization_bin+(self.action_space_len,)\n",
        "    #Initilize the Q value for all state-action pairs as 0\n",
        "    self.Q_tabular = np.zeros(Q_table_size)\n",
        "  \n",
        "  #Discretize the observations\n",
        "  def discretize_obs(self, obs):\n",
        "    discretized = list()\n",
        "    for i in range(len(obs)):\n",
        "      scaling = (obs[i] + abs(self.lowerbound[i])) / (self.upperbound[i] - self.lowerbound[i])\n",
        "      new_obs = int(round((self.discretization_bin[i] - 1) * scaling))\n",
        "      new_obs = min(self.discretization_bin[i] - 1, max(0, new_obs))\n",
        "      discretized.append(new_obs)\n",
        "    return tuple(discretized)\n",
        "  \n",
        "  # Choose the action\n",
        "  def select_action(self, obs):\n",
        "    #####explain why we need the first part to randomly pick actions\n",
        "    if (np.random.random() < self.exploration_rate):\n",
        "      return self.env.action_space.sample() \n",
        "    else:\n",
        "      #choose the action\n",
        "      ###### Your Code starts there\n",
        "      \n",
        "      ###### Your Code ends there\n",
        "\n",
        "  # Update Q table\n",
        "  def update_the_Q_table(self, state, action, reward, new_state):\n",
        "    ###### Your Code starts there\n",
        "      \n",
        "    ###### Your Code ends there\n",
        "  def get_exploration_rate(self):\n",
        "    return self.exploration_rate*self.exploration_decay_rate\n",
        "\n",
        "  def train(self):\n",
        "    # you should collect all episodic rewards during training\n",
        "    #\n",
        "    reward_traj = []\n",
        "    for e in range(self.num_episodes):\n",
        "      ###### Your Code starts there\n",
        "      \n",
        "      ###### Your Code ends there\n",
        "\n",
        "    print('Finished training!')\n",
        "    return reward_traj"
      ],
      "metadata": {
        "id": "fose2cSXaKZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Cart_Pole_Q_agent()\n",
        "rwd_traj = agent.train()\n",
        "plt.plot(range(len(rwd_traj)),rwd_traj)\n",
        "plt.xlable('Episodes')\n",
        "plt.ylable('Rewards')"
      ],
      "metadata": {
        "id": "7_5iUx4leVXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the policy\n",
        "import gym\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "video = wrap_env(env)\n",
        "obs = env.reset()\n",
        "###### Test the performance of your trianed agent \n",
        "###### Your Code starts there\n",
        "      \n",
        "###### Your Code ends there\n",
        "video.close()            \n",
        "env.close()\n",
        "show_video()\n",
        "#right click the video, you can download it!"
      ],
      "metadata": {
        "id": "l1bXBrYwulWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### Your Code starts there\n",
        "      \n",
        "###### Your Code ends there"
      ],
      "metadata": {
        "id": "TyR_YxjryCPf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}